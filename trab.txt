Titulo: Estudo em profundidade sobre sobre programação concorrente: Uma breve análise 
sobre abordagens modernas para programação concorrente e assincrona.


Introdução: 
        Hoje em dia é cada vez mais comum que sistemas operem de maneira concorrente e possivelmente
        assincrona. Dos websites que precisar lidar com chamadas de api aos servidores que precisam
        lidar com milhares de clientes conectados ao mesmo tempo, noções de técnicas modernas para lidar
        com a complexidade de tais sistemas são extremente importantes no dia a dia do desenvolvedor. O objetivo
        desse trabalho é apresentar alguns problemas comuns do uso de threads para programação concurrente e algumas 
        de suas possiveis soluções, com um foco em problemas causados por operações bloqueantes de IO concorrentes. 
        
        

Motivação:
        Na aula "O que é um thread" foi feita a seguinte pergunta: "Porquê ainda se ensina programação sequencial?".
        Acredito que esse é um excelente questionamento. Considerando os pontos levantados em aula a respeito do carater
        cada vez mais paralelo dos softwares modernos e da inadequação do modelo puramente sequencial, a motivação desse trabalho
        parte do questionamento: "Porquê ainda não se ensina programação assincrona?". Além disso, a intenção
        é apresentar algumas possiveis  abordagens modernas de programação assincrona e concorrente. 




Threads e suas limitações: Exemplo do apache HTTP server 

        Para exemplificar algumas das limitações do uso de threads em situações onde
        é necessario realizar um grande número de operações concorrentes e possivelmente bloqueantes
        podemos estudar as limitações de um software que utiliza principalmente processos e 
        threads em sua implementação: O apache HTTP server, ou someone apache. 

        O apache é um software de código aberto que funciona como servidor web, executando
        código ou hospedando arquivos estaticos e retornando os resultados para clientes conectados. 
        O Apache, criado em 1995, ainda hoje é a opção mais popular de servidor, servindo aproximadamnete 25% dos
        sites mais movimentados da internet. É importante notar que sendo um servidor web, ele tem a necessidade de 
        gerenciar milhares de conexões simultaneas, e isso involve abertura de um socket e suas respectivas operações
        de leitura e escrita e a ṕossivel leitura e escrita e execução de arquuivos em disco. t 
                O apache oferece dois modos de funcionamento (Em sua versão 1.x):
            1 - "(pre)forked" : O apache cria um novo processo por meio de um fork() para
            cada nova conexão; Cada novo processo roda uma thread, e cada thread serve um usuario. O servidor
	    tenta manter uma "pool" de processos rodando mesmo sem usuarios conectados.
	    
	    2 - "worker" ; O apache cria um novo processo, e cada processo pode ter diversas threads. E cada
	    nova conexão é servida por uma thread dentro do processo, sendo uma conxão por thread
	
	De qualquer maneira, em ambos os seus modos o apache sempre executa uma thread por conexão. Sendo assim,
	conforme o numéro de conexões simultaneas vai crecendo, o apache começa a sofrer dos seguintes problemas:

	1) Por padrão, cada thread tem sua area de memoria para sua pilha (stack). O tamanho da pilha é configuravel,
	mas alterações em seu tamanho padrão podem ter resultados desagradaveis (como por exemplo um limite de recursão menor).
	Considerando que tamanho de pilha padrão nos sistemas operacionais modernos é de 2MB, um computador com 8GB seria capaz de
	suportar somente 4.000 conexões simultaneas (menos, considerando que outros processo tambem estarão utilizando memoria). 

	2) Dada a granda quantidade de threads, a quantidade de tempo perdida em trocas de contexto tende a ser 
	extremamente alta, já que o numero de threads é muito superior ao numero de cores do processador. Além disso
	a necessidade do compartilhamento e sincronização de recursos (incluindo operações bloqueantes) entre milhares 
	de threads aumenta muito a chance de possiveis deadlocks.

	3) A efieciencia do cache do processador é prejudicada. Em uma situação de multiplas threads por core
	cache misses vão ser muito mais comuns, principalmente pela questão da pilha das threads ocupar uma quantidade
	relativamente grande de memoria.

	Sendo assim, mesmo com computadores poderosos era extremamente dificil conseguir gerenciar muitas conexões simultaneas com 
	um servidor apache.

        A dificuldade geral de conseguir atigir uma grande quantidade de conexões simultaneas em um unico servidor foi bem exemplificada
        na epoca pelo conhecido problema do C10K, que se trata do problema de conseguir gerenciar 10,000 conexões simultaneas em um unico
	computador. Considerando que a 15 anos atŕas o apache era de longe o servidor mais popular, é possivel argumentar um dos fatores 
	para esse problema era justamente o modelo de threads do apache. É importante notar que hoje em dia o apache2.0 oferece outros modos
	de operação que não são baseados em uma thread por usuario.

Soluções para o problema do C10K: Epoll e IO não bloqueante:
	Algumas soluções a nivel de sistema operacional foram desenvolvidas para lidar com o problema da concorrencia. 
	No geral, a ideia é que o sistema operacional não vai suspenda a thread em operações de IO, em vez disso optando
	por devolver imediatamente a quem chamou a função uma handle que permite verificar de alguma maneira se a operação
	está pronta para ser executada de maneira não bloqueante.

	No Unix para tal foi implementada a api epoll. epoll e suas operações relacionadas
	relaciondas permitem registrar multiplos file descriptors para serem "observados":
	    - epoll_create(u32 size) -> Cria uma nova instancia de um epoll. retorna um file descriptor referente a instancia criada
	    - epoll_ctr(int epfd, int op, int fd, struct epoll_event *event) -> adiciona, modifica ou remove eventos em uma instancia
	de epoll. epfd se refere a instancia do epoll, op a operação (adicionar, remover ou modificar), fd ao file descriptor que será observado
	e *event a struct com os eventos a observar no file descriptor cadastrado e com o dado que deve ser retornada quando o evento ocorrer.
	    - epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) -> Espera por qualquer um dos eventos registrados, com
	um timeout opcional. Na ocorrencia de qualquer evento, a função retorna o dado passado na struct epoll_event.
	
	Com o epoll é possivel definir um conjunto de eventos (Ex: socket pronta para leitura) que devem ser observados para 
	cada file descriptor, e um dado a ser retornado quando esse evento ocorre. Por exemplo, podemos cadastrar para cada file descriptor
	referente as sockets abertas do computador um struct evento de "Pronto para leitura" com um ponteiro para o fd
	que está pronto para a leitura dentro. E então em uma unica thread, executar em loop a função epoll_wait() para
	processar sequencialmente toads os sockets conforme suas operações bloqueantes forem ficando disponiveis. 
	
	Sendo assim, o epoll permite que uma unica thread processe um numero muito grande de operações concorrentemente.
	Além disso o fato de que é póssivel ter um 'aviso' de que alguma computação normalmente bloqueante está pronta
	permite a construção de diversas abstrações para representar computações que não ocorrem de maneira sincrona. 

	Em comapração com o apache, o servidor/proxy Nginx utiliza uma abordagem baseada em loops de eventos com 
	epoll(UNIX)/kqueue(BSD)/IOCP(Windows), permitindo sem maiores dificuldades tantas conexões quanto o limite de file descriptiors do sistema. 
	Uma unica instancia do nginx em um processador single core padrão consegue facilmente lidar com centanas de milhares de conexões em uma unica thread.

Concorrencia e funções assincronas: 
	Simplificadamente, funções assincronas represetam operações que tem (no minimo) no dois estados: Um estado de 'pendente'
	e um estado de 'completa'. Ao realizar um chamado de função assincrona, a função inicia no estado pendente e imediatamente
	retorna uma handle que permite consultar o seu estado atual. Quando a função completa sua operação longa e bloqueante, 
	seu estado passa para 'completa', e o programa pode recuperar seu resultado, sendo possivel então deferir a espera
	do resultado da função para algum momento mais oportuno.
	
	A api Epoll tem a caracteristica de transformar uma operação bloqueante em uma operação assincrona, mas
	é bom notar que o epoll é apenas uma parte e uma das diversas maneiras de implementar funções assincronas. 
	É comum que o epoll seja utilizado como base de abstrações de nivel mais alto para funções assincronas como
	tasks, promisses, futures, usermode threads (green threads) e corotinas. Além disso, normalmente funções assincronas
	são executadas por um escalonador implementado no proprio programa programa em cima de uma threadpool com uma thread por 
	core (2 com hyperthreading).  


Conclusão:


